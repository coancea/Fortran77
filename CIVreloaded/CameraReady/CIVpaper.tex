\documentclass{sig-alternate}



\usepackage{makeidx}

%\newenvironment{titemize}{
%    \vspace{-.4\baselineskip}
%    \begin{itemize}
%    \itemsep -.1\baselineskip
%}
%{\end{itemize} \vspace{-.3\baselineskip}}
%\newcommand{\titem}{\item[$\bullet$]}
%

\usepackage{amsmath}
\usepackage{amssymb}
%\usepackage{amsthm}

\usepackage{multicol}
\usepackage{multirow}
\usepackage{graphicx}
\usepackage{color}
\usepackage{subfigure}
\usepackage{epsfig}
\usepackage{fancybox}
\usepackage{fancyvrb}

\usepackage{rotating}

\usepackage{hyperref} % comes last, as it redefines a couple of commands
\hypersetup{
    colorlinks,%
    citecolor=black,%
    filecolor=black,%
    linkcolor=black,%
    urlcolor=blue
}

\DefineVerbatimEnvironment{colorcode}%
        {Verbatim}{fontsize=\small,commandchars=\\\{\}}   % \normalsize
        %{Verbatim}{fontsize=\scriptsize,commandchars=\\\{\}}


\definecolor{DikuRed}{RGB}{130,50,32}
\newcommand{\emp}[1]{\textcolor{DikuRed}{ #1}}
\definecolor{CosGreen}{RGB}{10,100,70}
\newcommand{\emphh}[1]{\textcolor{CosGreen}{ #1}}


\newcommand{\mymath}[1]{$ #1 $}
\newcommand{\myindx}[1]{_{#1}}
\newcommand{\myindu}[1]{^{#1}}
\newcommand{\mymathbb}[1]{\mathbb{#1}}

\newcommand{\Figures}[1]{../Figures/}

\hyphenation{in-de-pen-dence}
\hyphenation{pa-ra-lle-lize}
\hyphenation{sub-scrip-ted}
\hyphenation{sub-scrip-ted}
\pagestyle{plain}


\begin{document}

%\conferenceinfo{ICS}{'13 Eugene, Oregon USA}


%\title{ Summarizing Accesses Without Closed-Form Solutions in Loop : Beyond Idiom Recognition }   
\title{Scalable Conditional Induction Variables (CIV) Analysis}   

\numberofauthors{2}

\author{
\alignauthor
Cosmin E. Oancea\\
       \affaddr{Department of Computer Science}\\
       \affaddr{University of Copenhagen}\\
       \email{cosmin.oancea@diku.dk}
% 2nd. author
\alignauthor
Lawrence Rauchwerger\\
       \affaddr{Department of Computer Science and Eng.}\\
       \affaddr{Texas A \& M University}\\
       \email{rwerger@cs.tamu.edu}
}


\maketitle
%\thispagestyle{empty}


\pagenumbering{arabic}

\begin{abstract}

Subscripts using variables, named {\sc civ}, that cannot be expressed 
as a formula in terms of the enclosing-loop indices,  
appear in the low-level implementation of common programming abstractions 
such as filter operations or stacks or vectors in which elements are 
inserted at the end, and pose significant challenges to automatic
parallelization.

This paper presents a flow-sensitive technique that summarizes 
both {\sc civ}-based and affine subscripts to program level,
and under the same representation: Our technique (i) requires no 
modifications of the dependency test, which is agnostic to the 
original shape of the subscripts, and (ii) may prove useful
in a wider context, e.g., array {\sc ssa}. 
%, and may be applied in a wider context, e.g., array {\sc ssa}.
%
%On the one hand, our technique does not require modifications of 
%the dependency test, which is agnostic to the original shape of 
%the subscripts. 
In comparison, related work uses specialized dependency tests that 
%operate at the level of each pair of read-write subscripts, and
disambiguate each pair of read-write accesses (locally), and 
have been found less-suited than the summary-based ones in analyzing 
more complex loops. 
%than the ones based on summaries.   
%On the other hand, our technique is not restricted to 
%dependency tests and may be applied for example in the context 
%of array {\sc ssa}.
%
We report a systematic evaluation of five Fortran benchmarks that demonstrates 
  (i) efficient (scalable) parallel execution, 
 (ii) that important loops exhibiting {\sc civ} subscripts are not infrequent, and 
(iii) that our techniques covers loops that were either not reported or were 
        previously solved each with a different technique.
\end{abstract}


\category{D.1.3}{Concurrent Programming}{Parallel Programming}
\category{D.3.4}{Processors}{Compiler}


\terms{Performance, Design, Algorithms}

\keywords{
array-reference summaries, conditional induction variables}


\section{Introduction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% CLASSICAL ANALYSIS: Poly/Array Indexing + CIV + High-Level Contrib %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\enlargethispage{\baselineskip}

%Dependency analysis~\cite{BanerjeeIneqTest,FeautrierDataflow,Pugh92theomega} 
%of loop nests for the purpose of automatic parallelization requires analysis of 
%array subscripts that often use induction variables. %The latter

Automatic parallelization of loops typically requires analysis of 
induction variables, which commonly refer to scalars that are 
(de)incremented with the same amount in each iteration of a loop or 
are an affine function of another induction variable. 
%
While induction variables communicate their updates across  %recurrence
iterations, they can still be parallelized by substituting their 
occurences with a closed-formed expression of the loop-nest indices. 
%
This furthermore enables dependency analysis~\cite{BanerjeeIneqTest,FeautrierDataflow,Pugh92theomega} 
of arrays, which typically assumes that subscripts are affine 
expressions of loop-nest indices.
% 

\begin{figure}
\begin{minipage}{0.58\columnwidth}
\begin{colorcode}
k = k0     \emphh{\em Ind.}  k = k0        
DO i =1,N  \emphh{\em Var.}  DO i = 1, N      
  k = k+2    \emphh{\mymath{\Rightarrow}}    a(k0+2*i)=.. 
  a(k)=..  \emphh{\em Sub.}  ENDDO         
ENDDO            k=k0+MAX(2N,0)

   (a)               (b) 

\end{colorcode}
\end{minipage}
\begin{minipage}{0.35\columnwidth}
\begin{colorcode}
DO i = 1, N
 IF(cond(b(i)))THEN 
    civ = civ + 1 \emp{\mymath{\Rightarrow}{\em ?}} 
    a(civ) = ...
ENDIF ENDDO

      (c)

\end{colorcode}
\end{minipage}
\hrule
\caption{Loops with affine and {\sc civ} array accesses.}
\label{fig:introEg}
\vspace{-2ex}
\end{figure}


For example, the loop in Figure~\ref{fig:introEg}(a) increments
by two the value of {\tt k} (produced by the previous iteration),
%variable {\tt k} by two in every iteration {\tt i}, 
and updates the {\tt k$^{th}$} element of array {\tt a}. 
%
The uniform incrementation of {\tt k} allows one to summarize and
substitute {\tt k} with {\tt k0+2*i} in the body of the loop, 
resulting in the code in Figure~\ref{fig:introEg}(b).
%
This substitution eliminates the cross-iteration dependency on {\tt k}, 
and allows to easily verify that the set of points written by any
two distinct iterations {\em do not overlap}, a.k.a., output independence: 
{\tt k0+2*i$_1$~=~k0+2*i$_2$ $\Rightarrow$ i$_1$=i$_2$}. 
It follows that the loop in Figure~\ref{fig:introEg}(b) can be safely
parallelized. 

\enlargethispage{\baselineskip}

A known difficulty in performing this analysis arises when
subscripts use scalars that resemble induction variables, except 
that they are (de)incremented differently or not at all in 
different iterations of the loop.  We call such variables 
{\em conditional induction variables} or {\sc civ} for short,
because they are typically updated  (only) on some of 
the possible control-flow paths of the iteration.
%
For example, the loops in Figures~\ref{fig:introEg}(a)~and~(c) are 
similar except that in (c) both {\tt civ} and the array update are 
performed only when condition {\tt cond(b(i))} holds.  
Although the recurrence that computes {\tt civ} values can still 
be parallelized via a scan~\cite{segScan} (prefix sum) precomputation, 
neither {\sc civ} nor the subscript can be summarized as affine 
expressions of loop index {\tt i}, and hence the previous technique 
would fail.  
% 
{\em This paper} presents a novel induction-variable analysis that allows
both affine and {\sc civ} based subscripts to be effectively summarized
(and analyzed) at program level into {\em one common representation}.


Current solutions~\cite{Blume94RangeTest,SeqVars,VEG,PaduaDemDrInterproc,CohenBeyondMon} %PaduaStackArr,
% Blume95symbolicrange 
use a two-step approach:
First, {\sc civ} scalars are recognized and their properties, such as 
monotonicity or cross-iteration distance, are inferred.   
Intuitively, in our example, this corresponds to determining that 
the values of {\tt civ} are increasing within the loop with step 
at most one.
This paper does not contribute to this stage, but  
exploits previously developed techniques~\cite{VEG}.

Second and more relevant, each pair of read-write subscripts is ({\em locally}) 
disambiguated, by means of {\em specialized} dependency tests that exploit
{\sc civ}'s properties. 
In our example, the cross-iteration monotonicity of the {\sc civ} values 
dictates that the {\sc civ} value, named {\tt civ$_2$} after increment 
in some iteration {\tt i$_2$} is strictly greater than the value,
named {\tt civ$_1$} of any previous iteration {\tt i$_1$}.
It follows by induction that the update of {\tt a(civ)}
cannot result in cross-iteration (output) dependencies, i.e.,
the system {\tt civ$_2$-civ$_1 \geq 1$} and {\tt civ$_2 = $civ$_1$}
has no solution. 
%Finally, the per-iteration {\sc civ} values can be (pre)computed 
%via a prefix-sum parallel operation, since {\tt +} is associative.

On the other hand, summarization techniques~\cite{SUIF,LMAD,CosPLDI} aggregate 
array references across control-flow constructs, e.g., branches, loops, and  
model dependency testing as an equation on the resulted abstract sets.
In practice, they were found to scale better than previously developed
analysis based on pairwise accesses~\cite{SUIF}, but do not support {\sc civ}s.

This paper proposes an extension to summary-based analysis %of memory references
that absorbs {\sc civ} subscripts into the same representation %summary 
as the one used for affine subscripts, and thus enables scalable analysis
while requiring no modification to the dependency test. % to remain unchanged and accurate. 
 %a summary based memory reference analysis.
%
The gist of our technique is to aggregate symbolically the {\sc civ} 
references on every control-flow path of the analyzed loop, in terms
of the {\sc civ} values at the entry and end of each iteration or loop.
The analysis succeeds if (i) the symbolic-summary results are identical 
on all paths and (ii) they can be aggregated across iterations %consecutive
as intervals.
%in the interval domain. 

We demonstrate the technique on the loop in Figure~\ref{fig:introEg}(c),
where we use {\tt civ$_\mu^i$} and {\tt civ$_b^i$} to denote the values of 
{\tt civ} at the entry and end of iteration {\tt i}, respectively.
%
On the {\tt THEN} path, i.e., {\tt cond(b(i))} holds, the write set 
of array {\tt a} is interval $W_i=${\tt [civ$_\mu^i$+1,civ$_b^i$]}, 
i.e., point {\tt \{civ$_\mu^i$+1\}}, because the incremented value 
of {\tt civ} is live at the end of the iteration. 
%
On the {\tt ELSE} path, the write set of {\tt a} is the empty set,
which is represented with the same interval as for {\tt THEN} path, 
if we consider for uniformity an interval empty whenever 
its lower bound is higher than its upper bound. 
%
%However, in order to obtain an unified representation we can 
%represent the empty set with an interval 
%in which the lower bound is higher than the upper bound.
This is true since {\tt civ} is not updated and so
{\tt civ$_\mu^i$} equals {\tt civ$_b^i$}, hence 
{\tt civ$_\mu^i$+1$ > $civ$_b^i$}.
%
%but is expressible with the same symbolic formula 
%$W_i=${\tt [civ$_\mu^i$+1,civ$_b^i$]} under the convention 
%that an empty interval has its lower bound greater than its upper bound. 
%(We have used that, since {\tt civ} is not incremented, {\tt civ$_\mu^i$} 
%equals {\tt civ$_b^i$}, hence {\tt civ$_\mu^i$+1$ < $civ$_b^i$}).

Aggregating $W_k$ across the first {\tt i-1} iterations results in
%$\cup_{k=1}^{i-1}W_k \ = \ ${\tt [civ$_\mu^1$+1,civ$_b^1$]~$\cup\ldots\cup$~[civ$_\mu^{i-1}$+1,civ$_b^{i-1}$]} $\ = \ $ {\tt [civ$_\mu^1$+1,civ$_\mu^i$]}, where we have used {\tt civ} values'
$\cup_{k=1}^{i-1}W_k \ = \ $ {\tt [civ$_\mu^1$+1,civ$_\mu^i$]}, 
where we have used {\tt civ} values'
monotonicity and the implicit invariant {\tt civ$_b^{k-1}\equiv$civ$_\mu^{k}$},
i.e., the {\tt civ} value at an iteration end is equal to the {\tt civ}
value at the entry of the next iteration. 
%
%Since in many cases it is not possible to derive an accurate result,
%our analysis computes a pair of under/overestimate summaries, which
%are enough in practice to verify the needed invariants.
%
As a last step, program invariants are modeled as abstract-set equations
that do not depend on the original-subscript shape, i.e., 
{\sc civ}-based or affine. 
%
In our example, the output independence equation:\\
$(\cup_{k=1}^{i-1}W_k)~\cap~W_i = ${\tt [civ$_\mu^1$+1,civ$_\mu^i$]}$\cap$
{\tt [civ$_\mu^i$+1,civ$_b^i$]}$ = \emptyset,~\forall \mbox{\tt{}i}$,
verifies that the write set of any iteration {\tt i} does not overlap
the write set of all previous iterations, and, by induction,
that no distinct iterations write a common element of {\tt a}.

\enlargethispage{\baselineskip}


Finally, we report that our technique is well integrated % fully implemented and 
in a variant of Polaris~\cite{Blume94automaticdetection} 
{\tt Fortran} compiler that uses {\em hybrid analysis} to
verify dataset-sensitive invariants % (statically-unverifiable)  
at runtime by evaluating a cascade of sufficient-condition 
predicates of increasing complexity, until one succeeds. 
(Predicates are statically synthesized from the summary 
equation that models the desired invariant.)
%
Our existing hybrid analysis does not require modifications
but directly benefits from our technique: 
%
First, our analysis simplifies {\sc civ}-summary expressions
enabling successful verification of the desired invariants.
%to a form that enables successful verification of the desired invariants. 
Second, our analysis extracts the slice of the loop that computes
the {\sc civ} values and evaluate it in parallel before loop
execution. This allows: 
%Second, the (parallel) pre-computation of {\sc civ} values allows:
  (i) runtime verification of {\sc civ} monotonicity whenever 
          this cannot be statically established, 
 (ii) sufficient conditions for safe parallelization to 
          be expressed in terms of the {\sc civ} values at loop's 
          entry, end, and anywhere in between, and 
(iii) safe parallelization in the simple case when
        {\sc civ} are not used for indexing.
The latter is not reported in related work.
%
Third, {\sc civ} summaries have a wider applicability than 
dependency testing, e.g., in the context of array {\sc ssa}.
%
%{\bf This paper makes the following contributions:}
In summary, main contributions are:
\begin{itemize}
    \item A flow-sensitive analysis that extends %(generalizes) 
            program level summarization to support {\sc civ}-based 
            indices and leads to the parallelization of
            previously unreported loops, e.g., the difficult 
            loop {\tt EXTEND\_do400} from {\tt track},
%by meaningfully combining the control-flow 
%of the summary with the one of the {\sc civ}.
%            and disambiguates loops that were not reported 
%            before or were solved each with a different technique,

    \item A non-trivial code generation that separates the 
            program slice that performs the parallel-prefix-sum
            evaluation of the {\sc civ} values 
            from the main (parallel) computation in which 
            they are inserted. To our knowledge, previous
            solutions do not support this, albeit it is 
            necessary at least when {\sc civ}s are not used for indexing.
            

    \item An evaluation of five difficult to parallelize benchmarks 
            with important {\sc civ}-subscripted loops, which measures 
            all runtime overheads and shows application level speed-ups 
            as high as $7.1\times$ and on average $4.33\times$ on $8$ cores.
%            This includes all runtime overheads, 
%            e.g., {\sc civ}-values computation, which are either 
%            negligible or scale well with the degree of parallelism.
%
\end  {itemize}



%Speedups of two of the benchmarks shown
%in this paper were also reported by~\cite{CosPLDI}, however
%the compiler techniques to achieve these results were 
%never presented, perhaps due to lack of space.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% STRUCTURE OF THE PAPER %%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%The rest of the paper is structured as follows: Section~\ref{subsec:Background}
%provides an introduction of our overall compiler framework.
%Section~\ref{sec:Core} presents in detail our solution to extracting
%predicates that disambiguate non-linear accesses.
%Section~\ref{sec:RelWork} compares against 
%related solutions, 
%Section~\ref{sec:EmpEval} evaluates our approach on  
%a set of benchmarks rich in non-linear accesses, and
%Section~\ref{sec:Concl} concludes the paper. 


\section{An Intuitive Demonstration}
\label{Intro:RelAppLim}

\enlargethispage{\baselineskip}

Figure~\ref{fig:codeActforCorrec}(a) shows a simplified version of
loop {\tt CORREC\_do401} from {\sc bdna} benchmark, %(\textsc{PerfectClub}),
as an example of non-trivial loop that uses both {\sc civ} and affine-based
subscripts.   The {\tt civ} variable uses (gated) 
single-static-assignment ({\sc ssa}) notation~\cite{GatedSSA}:
%
For example, the statement {\tt civ@2=$\gamma$(i.EQ.M,civ@1,civ@4)}
has the semantics that the fresh variable {\tt civ@2} takes, depending 
on the evaluation of {\tt i.EQ.M}, either the value of {\tt civ@1} for 
the first iteration of the loop that starts at {\tt M}, or the value of 
{\tt civ@4} for all other iterations. For simplicity the gates are
omitted in the figure, and this section uses only read and write sets 
of array references, while in practice we maintain read-only, 
write-first and read-write sets. 

\begin{figure}
\begin{minipage}{0.48\columnwidth}
\begin{colorcode}
civ@1 = Q
DO i = M, N, 1
 civ@2=\mymath{\gamma}(civ@1,civ@4)
 .. = {\bf X}(i) ..
 IF C(i) .GT. 0 THEN
  DO j = 1, C(i), 1
   IF(..){\bf{}X}(j+civ@2       )=..
   IF(..){\bf{}X}(j+civ@2+  C(i))=..
   IF(..){\bf{}X}(j+civ@2+2*C(i))=..
  ENDDO
  civ@3 = 3*C(i) + civ@2
 ENDIF
 civ@4=\mymath{\gamma}(civ@3,civ@2)
ENDDO
civ@5=\mymath{\gamma}(civ@4,civ@1)  

            (a)
\end{colorcode}
\hspace{2ex}\includegraphics[width=1\textwidth]{\Figures/CorrecRWsets}
\end{minipage}
\begin{minipage}{0.45\columnwidth}
\begin{colorcode}
 VEG annotated with CIV-based 
 summaries at Gated-SSA Path, 
  Iteration and Loop Levels
\end{colorcode}
\includegraphics[width=1.21\textwidth]{\Figures/VEG_CORREC}\vspace{-2ex}
\begin{colorcode}
                (b)
\end{colorcode}
\end{minipage}
\hrule
\caption{(a) Loop {\tt CORREC\_do401} and (b) its VEG (Value Evolution Graph) 
                from {\sc bdna} ({\sc Perfect-Club}).}
\label{fig:codeActforCorrec}
\end{figure}

Parallelizing loop {\tt CORREC\_do401} in Figure~\ref{fig:codeActforCorrec}(a)
requires verifying two invariants: The first refers to disproving 
cross-iteration true and anti dependencies. For such dependencies to occur it 
is necessary that there exists a memory location that is both read and written,
and we plan to disprove this by verifying that the overestimates of the read 
and write sets (of subscripts) of {\tt X}, aggregated at the outer-loop level, 
do not overlap. 
%
The second invariant refers to disproving output dependencies, which reduces to 
showing that the write-set overestimates of any two iterations do not overlap. 

Matters are simple for summarizing the affine 
read access {\tt X(i)}: since {\tt i} is the loop index ranging from {\tt M} to {\tt N}, the
read set across the loop is  {\tt$\lceil R \rceil =\cup_{i=M}^{N}\{i-1\}=$[M-1,N-1]},
where we have used the convention that array indices start from $0$ 
(rather than $1$ as in the introductory example). 

Computation of the write-set {\em overestimate} starts by summarizing 
the inner loop: the {\tt if} branches are conservatively assumed as taken, 
and  the three affine inner-loop updates  are summarized to interval
{\tt[civ@2,civ@2+3*C(i)-1]} by expanding index {\tt j} to its range 
{\tt [1,C(i)]}, e.g., access {\tt X(j+civ@2)} generates the interval 
of subscripts {\tt[civ@2,civ@2+C(i)-1]}.   

Similar to the introductory example, outer-loop aggregation
analyzes each path of an iteration:
On the path on which {\tt C(i).GT.0}, we have %that 
{\tt civ@4$=$civ@3$=$civ@2+3*C(i)}, and the path summary is
rewritten as {\tt [civ@2,civ@4-1]}, i.e., in terms of the {\tt civ}
variables that start and end of an iteration. The other path neither 
updates {\tt X} nor increments {\tt civ}, hence it accepts 
{\em the same symbolic summary} {\tt[civ@2,civ@4-1]$=\emptyset$}, 
because {\tt{}civ@2=civ@4>civ@4-1} and an interval that has its lower 
bound greater than its upper bound is empty.
%
It follows that the outer-loop iteration summary is
{$W_i$~=~[civ@2,civ@4-1]}.

Since {\tt civ} is updated only when {\tt C(i)} is positive,
its values are monotonically increasing within the loop with
the lower and upper bounds being {\tt civ@1=Q} and {\tt civ@5}. 
It follows that the write-set overestimate of the outer loop is 
interval $\lceil W \rceil = \lceil \cup_{i=M}^{N}W_i\rceil =${\tt[Q,civ@5-1]}.
Flow independence requires   
$\lceil W\rceil\cap\lceil R \rceil \equiv {\tt [M-1,N-1]}\cap{\tt{}[Q,civ@5-1]}=\emptyset$,
and a sufficient condition for this equation to hold is (easily) extracted:
${\tt Q} \ge {\tt N}~\vee~{\tt M} > {\tt{}civ@5}$.
%
Finally, output independence is proven statically, similarly to the 
{\sc civ} loop of Figure~\ref{fig:introEg}.

\vspace{1ex}
We conclude this section with {\em two high-level observations}: %  
{\em First}, our analysis summarizes {\sc civ} and affine subscripts
much in the same way, and enables a dependence test which is agnostic
to the kind of subscripts that were used.
%
These properties do not seem to hold for related approaches: 
For example, techniques that disambiguate consecutively written,
single indexed subscripts~\cite{PaduaDemDrInterproc,VEG} cannot solve
{\tt CORREC\_do401} because the accesses do not follow such pattern.
Similarly, a technique~\cite{CohenBeyondMon} that disambiguates 
pairs of accesses of shape {\tt \{X(civ),X(civ+CT)\}} may prove the output 
independence of {\tt CORREC\_do401}, but will not prove flow independence. 
%


{\em Second}, summary approximation has been key to successful
analysis. For example in the code in Figure~\ref{fig:codeActforCorrec}(a), 
overestimating the write set of the inner loop by assuming that all 
branches are taken allowed the {\sc civ} subscripts to be aggregated
as an interval. 
More importantly, the separation of the read and write sets cannot 
be verified statically: it requires runtime information. 
%
It follows that the accurate write-first summary at the 
outer-loop level cannot be statically simplified to an interval,
but its overestimate, consisting of only the {\sc civ} term, 
can and is sufficient for proving parallelization.

Finally, even if an affine (or different-{\sc civ})
{\em write access} is added to the code, the write overestimate
is still computable by separately aggregating each term and 
uniting the results.


\section{Preliminary Concepts}
\label{subsec:Background}


Our analysis of {\sc civ} subscripts builds on four main techniques: 
%reported in the literature, which we have implemented 
%in our compiler, but to which this paper does not claim any contributions:
%
First, a baseline analysis summarizes array indices into read-only~{\sc ro}, 
read-write~{\sc rw} and write-first~{\sc wf} abstract sets, using a 
representation named unified set reference~\cite{HybAn} ({\sc usr}).
%
Second, loop independence is then modeled as an equation in the 
{\sc usr} domain. 
%
Third, whenever this equation cannot be satisfied statically, a 
predicate is extracted from it, and its evaluation verifies independence 
at runtime~\cite{CosPLDI}. 
%
Fourth, the value evolution graph~\cite{VEG} ({\sc veg}) is 
used to model the flow of {\sc civ} values in a loop, and to 
query scalars' ranges.
The remaining of the section establishes a uniform notation and 
formulates the problem to be solved by {\sc civ} analysis. 


%\vspace{1ex}
\begin{figure}[hbt]
\hspace{-4ex}
\begin{tabular}{lclr}
$USR$ & ::= &  $LMAD$                 & ($n$-dim strided intervals)\\
      & {\tt~~}| & $USR \ \cup \ USR$ & (set union)\\
      & {\tt~~}| & $USR \ \cap \ USR$ & (intersection)\\
      & {\tt~~}| & $USR \ - \ USR$ & (difference)\\
      & {\tt~~}| & $Exp^{bool} \ \# \ USR$ & (gated USR)\\
      & {\tt~~}| & $_l\cup_{i=1}^{N} \ USR$ & (full recurrence)\\
      & {\tt~~}| & $_l\cup_{k=1}^{i-1} \ USR$ & (partial recurrence)\\
      & {\tt~~}| & $CallSite \ \bowtie \ USR$ & (callsite translation)
\end{tabular}%\vspace{-2ex}
\caption{Unified Set Reference (USR) Grammar.}
\label{fig:USRgrammar}
\end{figure}
%\vspace{1ex}

{\bf USR Construction}~\cite{HybAn}.
%
Summaries use the representation shown in Figure~\ref{fig:USRgrammar},
and can be seen as a {\sc dag} in which leaves correspond,
for simplicity, to strided (multi-dimensional) intervals, 
named linear memory access descriptors~\cite{LMAD} ({\sc lmad}) in the literature.
{\sc usr}'s internal nodes represent operations whose results are not
accurately representable in the {\sc lmad} domain: (i) irreducible set
operations, such as union, intersection, subtraction ($\cup$, $\cap$, $-$), 
or (ii) control flow: gates predicating
summary existence (prefixed by $\#$) or total ($_l\cup_{i=1}^N$) and partial 
($_l\cup_{k=1}^{i-1}$) unions corresponding to a loop $l$ of
index {\tt i=1,N}. 
%
We note that while {\sc usr}s are complex and accurate,
one can always under/over-estimate an {\sc usr} in the 
simpler strided-interval domain, albeit this may 
prove very conservative, i.e., $\emptyset$, or $[0,\infty]$. 


{\sc usr}s are built during a bottom-up traversal of a control-flow 
reducible program.
%
In this pass, data-flow equations dictate how summaries 
are initialized at statement level, merged across branches, 
translated across call sites, composed between consecutive 
regions, and aggregated across loops. 
%
For example, the aggregation equation for a loop $l$ of index 
{\tt i=1,N} is shown in Figure~\ref{fig:LoopAggreg}:
The loop-aggregated write-first set $WF$ is obtained 
by (i) subtracting from the write-first set of iteration 
$i$, i.e., $WF_i$, the reads of any iteration preceding $i$,
and (ii) by uniting the per iteration sets. 

\begin{figure}[t]
%\hrule ~
	\begin{tabular}{l r} \hspace{-5ex} 
	\multirow{2}{*}[22.5ex]
    {
   	  \subfigure[$\mbox{~~~~~~~~~~~~~~~~~~~~~}$]{ % Loop-Aggregation Equations
          \label{fig:LoopAggreg} 
		\makebox[0.4\textwidth][l] { \vbox{\scriptsize
SUMMARIZE($REG_i$, $i = 1, N$) \vspace{1ex} \newline
$\mbox{ }(WF_i, RO_i, RW_i) \leftarrow REG_i$ \vspace{1ex} \newline
$\mbox{ }R_i = RO_i \cup RW_i$ \vspace{2ex} \newline
$\mbox{ }WF = \bigcup_{i=1}^{N} (WF_i - \bigcup_{k=1}^{i-1}R_k)$ \vspace{1ex} \newline
%$\mbox{~~~~~~~~~~~~~~~~~~}\bigcup_{k=1}^{i-1}R_k)$ \vspace{1ex} \newline
$\mbox{ }RO = \bigcup_{i=1}^{N} RO_i - $ \newline %\bigcup_{i=1}^{N}(WF_i \cup RW_i)$ \vspace{1ex} \newline
$\mbox{~~~~~~~~~}\bigcup_{i=1}^{N}(WF_i \cup RW_i)$ \vspace{1ex} \newline
$\mbox{ }RW = (\bigcup_{i=1}^{N} R_i) - (WF \cup RO)$ \vspace{1ex} \newline
$\mbox{ }$RETURN $(WF, RO, RW)$ \vspace{1ex}
}
		}          			
	  } 
	} & { \hspace{-27ex}
	  \subfigure[$\mbox{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}$]{ %Independence Equations
          \label{fig:IndEq} 
		\makebox[0.57\textwidth][l] { \vbox{\scriptsize
OUTPUT INDEPENDENCE EQ:\vspace{1ex} \newline
$\mbox{ }\{ \cup_{i=1}^{N}(WF_i \cap (\cup_{k=1}^{i-1}WF_k))\} = \emptyset$ \vspace{4ex} \newline
FLOW/ANTI INDEPENDEP EQ:\vspace{1ex} \newline
$\mbox{ }\{(\cup_{i=1}^{N}WF_i) \cap (\cup_{i=1}^{N}RO_i)\} \mbox{ }\cup\mbox{ }$ \vspace{1ex} \newline
$\mbox{ }\{(\cup_{i=1}^{N}WF_i) \cap (\cup_{i=1}^{N}RW_i)\} \mbox{ }\cup\mbox{ }$ \vspace{1ex} \newline
$\mbox{ }\{(\cup_{i=1}^{N}RO_i) \cap (\cup_{i=1}^{N}RW_i)\} \mbox{ }\cup\mbox{ }$ \vspace{1ex} \newline
$\mbox{ }\{ \cup_{i=1}^{N}(RW_i \cap (\cup_{k=1}^{i-1}RW_k))\}=\emptyset$ \newline
%$\mbox{~~~~~~~~~~~~~~~~}=\emptyset$ \vspace{1ex}
}
		}
	  } 
	} 
\end{tabular}
\hrule
\caption{ (a) Loop Memory Reference Summary and (b) Loop Independence with Set Equations.}
\label{fig:UsrEq} %
\end{figure}

\vspace{1ex}

{\bf Independence Equations}. Figure~\ref{fig:IndEq} shows the {\sc usr}
equations, of form $S=\emptyset$, that model loop independence. 
The output independence equation
states that if for any $i$, the write-first set of iteration $i$
does not overlap with the write-first set of any iteration preceding
$i$, then, by induction, no two  iterations write the same % different
location. Similarly, for flow/anti independence it is checked 
(i) the disjointness of the total unions of the per-iteration {\sc wf}, 
{\sc ro}, and {\sc rw} set, and (ii) that the {\sc rw} sets of any two 
iterations do not overlap.

\vspace{1ex}

{\bf Synthesizing Independence Predicates}~\cite{CosPLDI}. %~\cite{SummaryMonot,CosPLDI}.  
When the independence equation
is not statically satisfied, we use a translation scheme $\mathcal{F}$,
from the {\sc usr} language to a language of % expressive
predicates, named {\sc pdag}, to extract a sufficient-independence condition: 
$\mathcal{F} : $ {\sc usr} $\rightarrow$ {\sc pdag},  
$\mathcal{F}(S) \Rightarrow S = \emptyset$. The result is then separated
into a cascade of predicates that are tested at runtime in the order of 
their complexities.  
If output dependencies cannot be disproved then privatisation is
necessary. If $WF_i$ is loop invariant then only the last iteration 
writes to non-private storage (static last value).

\vspace{1ex}

{\bf Value-Evolution Graph}~\cite{VEG} ({\sc veg}) is a {\sc dag}  %~\cite{VEG}
that represents the flow of values between gated-{\sc ssa}~\cite{GatedSSA} names 
of a scalar variable.  
%
{\sc veg}s are constructed on demand at loop and subroutine levels.
For example Figure~\ref{fig:codeActforCorrec}(a)~and~(b) shows 
our running example and the {\sc veg} describing the evolution of
variable {\sc civ} in the outer loop: %, respectively:
%
The entry node, e.g., merging cross-iteration values,  is named 
the $\mu$ node and is shown in Figure~\ref{fig:codeActforCorrec}(b) 
as a double ellipse. The recurrence, named the {\em back edge}, 
is shown as a dotted line, and the {\em back node} is drawn as a hexagon.
%
Regular nodes, drawn as ellipses, correspond to reduction-like 
statements. The {\sc veg} shows that our loop %{\tt CORREC\_do401} %This representation
uses a variable named {\tt civ} that is incremented on some %loop
paths with {\tt 3*C(i)}, and is unmodified on other paths. 

Note that branch conditions are not explicit in the {\sc veg}, 
but they are easily found from the gated-{\sc ssa} definition 
of $\gamma$-merged {\sc civ}s. % node ($\gamma$) . 
%
Nodes corresponding to arbitrary assignments, i.e., not a reduction, 
are named input nodes and are drawn in rectangles. 
%
A conditional induction variable ({\sc civ}) corresponds to a
particular {\sc veg}, in which:
  (i) the $\mu$ node is unique, dominates all other {\sc veg} nodes,
            i.e., is reachable from within the {\sc veg} only through its
            unique back edge, %on path that do not pass through other $\mu$ nodes,
%
 (ii) no node in the associated {\sc veg} can be reached via any input nodes, and 
%
(iii) the $\mu$-node values are provably monotonic.
            
Our running example complies with these rules:
For example, evolution {\tt 3*C(i)} in 
Figure~\ref{fig:codeActforCorrec}(b) is proven positive at the 
point of use, because path 
${\tt civ@2}\rightarrow{\tt civ@3}\rightarrow{\tt civ@4}$
is guarded by condition {\tt C(i).GT.0}, which, while not 
shown, is part of the gated-{\sc ssa} definition of $\gamma$-node {\tt civ@4}. 
%(as per the gated-{\sc ssa} definition of $\gamma$-node {\tt civ@4}).
If this is not possible statically, analysis can optimistically
assume monotonicity, and verify it at runtime in the slice 
that pre-computes the {\sc civ} values. 

\vspace{1ex}

{\bf Notation and Problem Statement.}
%\label{subsec:ProblemHL}
%
We denote by $\mathcal{L}$ a normalized loop of index $i\in\{1..N\}$ that
exhibits subscripts using {\sc civ} variables.
%
We denote by $U_i$ one of the write-first ({\sc wf}), read-write ({\sc rw})
or read-only ({\sc ro}) set-expression summaries of some array {\tt X} 
corresponding to iteration $i$ of loop $\mathcal{L}$, where the {\sc usr}'s 
leaves, i.e., {\sc lmad}s, may use {\sc civ} variables. 
%
For simplicity, we consider {\sc lmad}s to be 
strided intervals, i.e., $[l,u]^s=\{l, l+s, l+2*s, .., u\}$.
If $s=1$, we omit writing $s$; if the interval 
is a point $p$, we may write it $\{p\}$.

The goal is to compute symbolic under/over-estimates in the 
interval domain, denoted by $\lfloor A \rfloor$/$\lceil A \rceil$, 
for $U = \cup_{i=1}^{N} U_i$.
%
In the same way in which loop aggregation of an affine subscript  
eliminates the loop index $i$ from the result summary,
we declare {\sc civ}-{\em aggregation} across $\mathcal{L}$ {\em successful} 
if $\lfloor A \rfloor$ ($\lceil A \rceil$) does not depend on any loop-variant
symbols, e.g., any symbol in {\tt CIV}$_{\mu}$'s  {\sc veg}. %associated
%
For example, in Figure~\ref{fig:codeActforCorrec}(a), the affine
subscript of {\tt X(i)}, expressed as {\tt\{i-1\}}, is aggregated across 
the outer loop as {\tt[M-1,N-1]}, which is independent on {\tt i}. 
Similarly, we would like to overestimate the {\sc civ} write accesses 
across the loop as interval {\tt[civ@1,civ@5-1]} which does
not depend on loop-variant symbols, such as {\tt civ@3}.
%{\tt X(civ@3)}, expressed as {\tt\{civ@3-1\}}, across 
%the first inner loop as interval {\tt[0,civ@5-1]}.   ??????????????

In essence this would allow to treat {\em uniformly} and {\em compositionally} 
{\sc civ}-based and affine summaries: For example, the compiler can now 
establish flow independence by comparing the read and write summaries, 
which have the same representation, albeit the read and write sets
correspond to affine and {\sc civ}-based subscripts, respectively.
The other example refers to the difficult benchmark
{\tt track}, in which the output of the {\sc civ}-summarized (inner) loop 
{\tt EXTEND\_do500} becomes the input to {\sc civ}-based summarization 
of the outer loop {\tt EXTEND\_do400} (hence composable summarization).

%
Examining the terms appearing in the loop-aggregation and independence 
equations of Figure~\ref{fig:UsrEq}, we observe that we also need to similarly
reshape per-iteration ($U_i$) and partial-recurrence summaries 
($\cup_{k=1}^{i-1} U_k$), 
denoted by $A_i$ and $A_{k=1}^{i-1}$, respectively. 
%
Successful reshaping of $U_i$ means that $A_i$
depends only on the $\mu$ node, {\tt CIV}$_{\mu}^{i}$, 
and the back node, {\tt CIV}$_{b}^{i}$, of iteration $i$.
For $\cup_{k=1}^{i-1} U_k$ it means that $A_{k=1}^{i-1}$ depends
only on the {\tt CIV}$_{b}^{i-1}$ value of iteration $i-1$, which is
equal with the  {\tt CIV}$_{\mu}^{i}$ value of iteration $i$.
This allows to replace {\tt CIV}$_b^{i-1}$ with {\tt CIV}$_{\mu}^i$ in
$A_{k=1}^{i-1}$, and, as such, to compare $WF_i$ and $\cup_{k=1}^{i-1} WF_k$, 
e.g., an empty intersection implies output independence. 
%e.g., to prove that their intersection is empty, hence to establish %loop's 
%output independence.
%


\section{Monotonic CIV Summaries}
\label{sec:MonotonicCiv}

%The introductory example showed a relatively straightforward
%analysis that inferred an {\em accurate} summary for a loop 
%exhibiting {\em only} {\sc civ}-based subscripts.   
%and an exact interval-based
%summary may not even exist, regardless.

Our analysis is implemented as an extension of baseline 
summarization for the special cases of loop and iteration 
aggregation (and callsite translation), because at these 
levels the {\sc civ}-value properties, summarized by 
{\sc veg}, can be effectively used.
%
To compute the symbolic summary across all paths, the analysis
needs to combine the control-flow of the summary, e.g., 
encoded in {\sc usr}'s gates, with the control flow of the 
{\sc civ}, available in {\sc veg}. 
The key idea here is to conservatively 
%project the summary terms on the {\sc veg} 
associate summary terms with {\sc veg} nodes
and to merge across all {\sc veg} paths.
It follows that our analysis computes under and overestimate
summaries, but accuracy is recovered when the under and 
overestimate are identical.
%which also allows separate aggregation of the affine 
%and {\sc civ}-based terms (of different {\sc civ} variables). 
%

Over/under estimation also allows (separate) aggregation of the 
affine and {\sc civ}-based subscripts, which are typically
still accurate enough to verify the desired invariant.
%
%For example, the introductory example has shown a simple loop 
%with only (one) {\sc civ}-based subscripts that could be
%{\em accurately} summarized.
%
As with our running example, in practice, important loops use both %(benchmark) 
affine and {\sc civ}-based subscripts on the same array, hence 
terms of both kinds may appear in the set expression %formula 
of the accurate summary, which cannot be simplified
to an interval.

The remaining of this section is organized as follows:\\
%
\noindent{\em Section~\ref{subsec:BasicTechn}} presents the basic flow-sensitive 
analysis technique for summarizing over and underestimates, and 
demonstrates it on the running example depicted in 
Figure~\ref{fig:codeActforCorrec}.\\
%
\noindent{\em Section~\ref{subsec:Track}} shows several enhancements
to the basic technique that solve %allow automatic parallelization of %   
more difficult loops, such as {\tt EXTEND\_do400} from benchmark {\tt track}.
%
For example, the read-only (accurate) {\sc usr} in Figure~\ref{fig:Track}
can be simplified to point {\tt \{i-1\}} by filtering out the terms
belonging to the write-first underestimate.
%Section~\ref{subsec:Track}.
%
Finally, {\em Section~\ref{subsect:CivImplem}} details on the overall 
implementation, and focuses on how to (pre)compute safely, in parallel 
the {\sc civ}$_\mu$ values associated to each iteration.


\subsection{Basic Flow-Sensitive-Analysis Techinque}
\label{subsec:BasicTechn}

%({\sc lmad},{\sc lmad},{\sc lmad}) 
\begin{figure}[t]
\begin{small}
{\bf CIVSUM}($U_i :$~{\sc{}usr}) \vspace{1ex} \newline %{\bf raises Exception Fail}
$\mbox{ }\mbox{ }$// {\bf {\em Output:}} 
($A_i$, $A$, $A_{k=1}^{i-1}$) or {\bf Fail}s, i.e.,\newline 
$\mbox{\tt~}$//$\mbox{\tt~}$the under/over-estimate intervals of ($U_i$,$\cup_{i=1}^{N} U_i$, $\cup_{k=1}^{i-1} U_i$).\vspace{2ex}\newline
%($A_i \equiv U_i$, $A \equiv \cup_{i=1}^{N} U_i$, $A_{k=1}^{i-1} \equiv \cup_{k=1}^{i-1} U_i$) or {\bf Fail}s. \vspace{2ex} \newline
$\mbox{ }\mbox{ }${\bf1.} \textsc{Over/Under-estimate $U_i$ by a union of gated intervals} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ $(g_1\#L_1) \cup ... \cup (g_m\#L_m)\leftarrow U_i$: \vspace{2ex} \newline
$\mbox{ }\mbox{ }${\bf2.} \textsc{Associate each} $L_k, k\in\{1..m\}$, \textsc{to a VEG node} {\tt CIV@q}, \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            \textsc{which is the civ-immediate (post) dominator of}  \newline 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            \textsc{the program point where} $L_k$ \textsc{was summarized} \vspace{2ex} \newline 
$\mbox{ }\mbox{ }${\bf3.} {\bf For Each} \textsc{VEG path, symbolically unite (exactly) the}\newline 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            \textsc{~~~~~~~~~~~~~~$L_k$'s on that path into one interval $L_{path}$.}\vspace{1.5ex}\newline 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            {\bf If} $L_{path}$ \textsc{cannot be written in terms of the} {\tt CIV$_\mu$} {\sc and} 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            {\tt CIV$_b$} {\sc nodes} {\bf Then Fail}.\vspace{1.5ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            {\bf If} \textsc{in underestimate case} {\bf Then} \textsc{check that the condi-}
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
\textsc{tion of the VEG-$path$ implies $g_k$}. {\bf Else} $L_k \not\in path$.\vspace{2ex}\newline
$\mbox{ }\mbox{ }${\bf4.} {\bf If} \textsc{all $path$s have identical $L_{path}$} {\bf Then} $A_i = L_{path}$, \textsc{and}\newline 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
            \textsc{$A$ and $A_{k=1}^{i-1}$ are computed by exploiting the monoto-}\newline 
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$
\textsc{nicity and the symbolic bounds of {\tt CIV$_\mu$} {\sc and} {\tt CIV$_b$}.}\newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ {\bf Else Fail.}\vspace{1ex}
\end{small}
\hrule
\caption{ CIV-Based-Summarization Pseudocode.}
\label{fig:BasicTechnique} %
\end{figure}


The algorithm implementing the basic analysis is depicted in 
Figure~\ref{fig:BasicTechnique} and has four main stages: 
%The basic analysis consists of four main stages: 
First, under and overestimates of input {\sc usr} $U_i$ are 
computed under the form of a union of gated-interval pairs.   %{\sc lmad}s
Second, the intervals that use {\sc civ} variables are 
associated with nodes on the {\sc civ}'s {\sc veg} graph. %corresponding
Third, each {\sc veg} path is summarized via an interval expressed 
in terms of {\tt CIV$_\mu$} and {\tt CIV$_b$} nodes.\\
\noindent Finally, path intervals are merged across all paths, according to 
the under/over-estimate semantics, to yield the iteration-level interval $A_i$.
Total and partial-recurrence intervals, $A$ and $A_{k=1}^{i-1}$, 
are computed by suitably substituting {\tt CIV$_\mu$} and 
{\tt CIV$_b$} in $A_i$ with suitable ({\sc veg}) bounds,
which is safe due to the cross-iteration monotonicity of {\sc civ} values.
The remaining of this section details and demonstrates each algorithmic stage
on our running-example loop introduced in Figure~\ref{fig:codeActforCorrec}.

%\newcounter{taskCnt}
%\setcounter{taskCnt}{0}
%\newcommand{\nexttask}{\addtocounter{taskCnt}{1}\arabic{taskCnt}}
%
%\paragraph{\nexttask. USR Splitting} 
%

\vspace{1ex}


{\bf 1. Approximating an USR.}  % into a Union of Gated-Interval Pairs
For brevity, we do not present the algorithm for
over/under-estimating an {\sc usr} via 
{\em a union of gated-interval ${\tt g}\#L$ pairs}, where $L$ is an 
interval and {\tt g} is a condition predicating $L$'s existence.  
%
We note however that it is always possible to build an interval 
under/overestimate, by recursively pattern matching the {\sc usr}'s 
shape~\cite{SummaryMonot}, and enhancing the algorithm to also gather 
the gate information did not pose any significant challenges.
  
%
%Due to space constraints, we do not present here
%the algorithm used to rewrite an {\sc usr} into a union of 
%gated-interval ${\tt g}\#L$ pairs, where $L$ is an interval and 
%{\tt g} is a condition predicating $L$'s existence.  
%We just note that it is always possible to build an interval
%under/overestimate of an {\sc usr}, via a recursive pattern-matching of 
%the {\sc usr}'s shape~\cite{SummaryMonot}, and enhancing the algorithm to 
%also gather the gate information does not pose any significant challenges.   
%
%For example, with the (first inner) loop of index {\tt j} in Figure~\ref{fig:codeActforCorrec}(a), 
%the $WF_j$ summary of array {\tt X} is already in gated-interval form, i.e., 
%${\tt (IA(j).NE.0)}\#\{{\tt civ@3-1}\}$,
%hence the over and underestimates of $WF_j$ are identical. 
With the loop in Figure~\ref{fig:codeActforCorrec}(a), 
the write-first underestimate $\lfloor WF_i\rfloor$ of array {\tt X} 
is the empty set, because {\tt X} is (only) conditionally updated in 
the inner loop, and the gated overestimate is  
$\lceil WF_i\rceil={\tt{}(C(i).GT.0)}\#[{\tt civ@2, civ@2+3*C(i)-1}]$,
because the {\sc civ} accesses occur inside the inner loop, which
belongs to the {\tt THEN} target of statement {\tt IF(C(i).GT.0)}.
 
%due to the guarded write accesses in the loop of index {\tt j}.

\vspace{1ex}

{\bf 2. Associating Summaries with VEG Nodes.}
Our analysis uses the {\sc veg} to approximate the control-flow of
the loop, because (i) {\sc veg} describes the evolution of the 
{\sc civ}s used in subscripts, and (ii) it also retains the 
key control-flow information necessary for summarization. 

As such, we associate each gated-interval pair ${\tt g}\#L$ 
containing a {\sc civ}s with a node in the {\sc civ}'s {\sc veg}\footnote{  
In principle, we may have ``regular'' gated-interval pairs, and also 
different pairs may correspond to {\sc civ}s from different {\sc veg}s. 
These are treated independently and the result is the union of partial results.
Analysis fails if one interval exhibits two {\sc civ}s belonging
to different {\sc veg}s.}. 
This corresponds to identifying the {\sc civ} node (in {\sc veg})
that most-accurately describes the program point {\tt PP$_L$} 
where interval $L$ was summarized:
%
%One can observe that the naive choice of associating $L$ with the {\sc civ}
%node that is used in $L$ is inaccurate: For example the gated-interval pair
%corresponding to $\lceil WF_i \rceil$ in Figure~\ref{fig:codeActforCorrec}(b) 
%is $[{\tt civ@2+1, civ@2+3*C(i)}]$, but node {\tt civ@3} 
%is certainly a better choice than {\tt civ@2}, because associating
%it with {\tt civ@2} would imply that the $WF_i$ summary also belongs 
%to the path on which {\tt IA(j).NE.0} does not hold. 
%
We compute both the immediate {\sc civ}-node dominator and 
post-dominator of {\tt PP$_L$}, denoted by {\tt CIV}$_{d}$ and 
{\tt CIV}$_{pd}$, respectively, and chose {\tt CIV}$_{pd}$
as the associated node if {\tt CIV}$_{d}$ dominates {\tt CIV}$_{pd}$, 
and {\tt CIV}$_{d}$ otherwise. This ensures that the associated {\sc civ}
is the ``closest'' node that belongs to any path passing through {\tt PP$_L$}
(the reverse does not hold).

For our running example, the $\lceil WF_i\rceil$ overestimate of {\tt X}, 
namely {\tt [civ@2,civ@2+3*C(i)-1]}, is projected to node {\tt civ@3}, 
as shown in the annotated {\sc veg} depicted in Figure~\ref{fig:codeActforCorrec}(b).
%

For underestimate computation it is {\em not enough} to associate an interval $L$ 
to a {\sc civ} node in the manner presented before, because it is not guaranteed
that any path that passes through the {\sc civ} node would also pass through {\tt PP$_L$}.
%
%For example $L$ may summarize accesses of an inner-{\tt IF} branch 
%(scope) that does not update the {\sc civ} variable. It follows that the {\tt IF} 
%is not represented in the control-flow of the {\sc veg}, and it is incorrect
%to consider $L$ part of the underestimate since the {\tt IF} branch might not be 
%taken at runtime.
%
At this point we use the gate $g$ associated with $L$, which subsumes all the
conditions guarding the accesses summarized by $L$: 
%the inner-{\tt IF} condition: 
In the computation of an underestimate, a gated-interval $g\#L$ is considered 
part of a {\sc veg} path {\em iff the (total) condition of the {\sc veg} path}, 
i.e., the conjunction of all the $\gamma$-node gates on that path, {\em implies $g$}, 
i.e., the existence condition of $L$.  This guarantees that the summary belongs
to any control-flow path that includes the considered {\sc veg} path\footnote{
Establishing this invariant strictly from the control-flow graph is more
conservative, e.g., in our case the gates are translated/simplified across call sites,
hence more accurate.
%because the gates are aggressively simplified, e.g., across call sites, hence more accurate. 
}.
%With our code, $g={\tt IA(j).NE.0}$ is identical to the gate of the path 
%${\tt civ@2}\rightarrow{\tt civ@3}\rightarrow{\tt civ@4}$,
%hence interval {\tt \{civ@3-1\}} safely belongs to this path.


\enlargethispage{\baselineskip}

\vspace{1ex}

{\bf 3. VEG-Path Summarization.} 
%
To compute the result on {\em one} path, all intervals are rewritten in terms of only the
{\sc civ}$_\mu$ node, by using the symbolic formulas of the path's evolution.
Then, all intervals belonging to the {\sc veg} path are united (unioned). 
If this succeeds, i.e., the result is one interval, then the resulting-interval upper 
bound is written in terms of the back node, {\sc civ}$_b$. If the result is free of
loop-variant symbols (other than {\sc civ}$_\mu$ and  {\sc civ}$_b$), such as {\tt 3*C(i)},
then the path-level analysis succeeds, otherwise it fails.

For example, the path ${\tt civ@2}\rightarrow{\tt civ@3}\rightarrow{\tt civ@4}$ 
of the {\sc veg} in Figure~\ref{fig:codeActforCorrec}(b), exhibits loop-variant evolution 
{\tt 3*C(i)}, and results in path interval {\tt [civ@2,civ@2+3*C(i)-1]}. 
%
However, rewriting the upper bound in terms of the back %$\mu$ and 
{\sc civ} node, results in $[{\tt civ@2},{\tt civ@4-1}]$,
i.e., we have performed substitution {\tt civ@2$\leftarrow$civ@4-3*C(i)}
derived from that path's evolution.
%since on that path ${\tt civ@4}={\tt civ@2+3*C(i)}$. 
It follows that aggregation succeeds.
%
%Similarly, for the same path of the {\sc veg} in  Figure~\ref{fig:VegACTFOR}, the
%path result is $\{{\tt civ@2}\}$, which is rewritten as
% $[{\tt civ@2}, {\tt civ@4-1}]$, since on that path 
%${\tt civ@3}={\tt civ@2+1}$
%and ${\tt civ@3}={\tt civ@4}$.

The other {\sc veg} path exhibits empty summaries and $0$ evolution,
i.e., {\tt civ@2=civ@4}. 
It follows that {\tt [civ@2,civ@4-1] $\equiv \ \emptyset$}  
describes it correctly as well, i.e., an interval in which the upper bound 
is smaller than the lower bound is empty.
  It follows that all paths share the same per-iteration result, 
in which the only loop-variant terms are the $\mu$ and back {\sc civ} nodes, 
and analysis succeeds.


\vspace{1ex}

{\bf 4. Merge Across All Paths.} 
%
The last stage of the analysis is to compute the merge-over-all-paths result (interval).
%in a way consistent with the definitions of under and overestimate, respectively.
In our implementation the merge succeeds only when all path results are 
identical, which holds on both examples, e.g., $\lceil WF_i \rceil = {\tt [civ@2,civ@4-1]}$.
In the general case, one can compute the intersection/union over all paths as the  
under/overestimate result, respectively.
%Without loss of generality 
For simplicity, in the following, we assume that {\sc civ} values are monotonically 
increasing, and that the result's upper bound increases with the iteration number,
i.e., positive stride. 


We compute the total-union (loop) result as if we aggregate an affine access
across a loop whose lower and upper bounds are equal to the value of the 
{\sc civ} at the loop entry and exit, respectively.  
%
In our case this corresponds to replacing {\tt civ@2} and {\tt civ@4} 
with the {\sc civ} values at the loop entry and exit, in our case 
{\tt civ@1} and {\tt civ@5}, respectively, which results in
{\tt [civ@1, civ@5-1]}.   If the sign of the {\sc civ} factor in
the affine-{\sc civ} expression is positive, then {\sc civ}'s
monotonicity ensures that the overestimate is correct. 

For the underestimate, correctness requires checking that the
per-iteration result $A_i$ is contiguous (or overlapping) 
between any two consecutive iterations. If $A_i=${\tt[lb$_i$,ub$_i$]$^s$},
this corresponds to checking that {\tt ub$_{i-1}$+s $=$ lb$_i$},
i.e., the upper bound of iteration {\tt i-1} plus the stride equals ($\leq$) 
the lower bound of iteration {\tt i}.
%
The check uses the invariant that the {\sc civ} value  
at the end of an iteration, i.e., the back node, equals 
the {\sc civ} value at the entry of the next iteration,
i.e., the $\mu$ node.
%
If $\lfloor WF_i \rfloor$ would be {\tt [{\tt civ@2},{\tt civ@4}-1]$^1$},
this would correspond to replacing {\tt civ@4} with {\tt civ@2} in the upper 
bound and checking {\tt civ@2-1+1$=$civ@2}, which verifies statically. 
%
Note that if the per-iteration result is a point, then the stride is 
not set yet, and in this case the stride is set to the value that 
verifies the invariant, i.e., {\tt lb$_i$-ub$_{i-1}$}.
%For example, if in Figure~\ref{fig:codeActforCorrec}(a) we would have 
%{\tt civ@3=civ@2+2} instead of {\tt +1}, then the totally-aggregated
%result would have stride {\tt 2}, i.e., {\tt [civ@1,civ@5-1]$^2$}.

Similarly, to compute the partial-loop-aggregation result, i.e., $\cup_{k=1}^{i-1}$,
we replace the {\sc civ} $\mu$ with the {\sc civ} node at the entry of the loop,
and the back node with the {\sc civ} $\mu$ node of iteration $i$, 
which results in interval {\tt[civ@1,civ@2-1]}.

Finally, if the over and underestimate results are identical then 
the result is exact. 
The computed summaries allow now to prove the flow and output independence of the 
running example, as we have already seen in Section~\ref{Intro:RelAppLim}.

%\enlargethispage{\baselineskip}

\subsection{Enhancements to the Basic Technique}
\label{subsec:Track}


This section presents several refinements of the basic analysis,
that allow to parallelize several difficult loops, e.g., {\tt track}.
One such loop is {\tt EXTEND\_do400} whose body consists of inner loop
{\tt EXTEND\_do500}. Since even the simplified code
is too complex for paper presentation, Figure~\ref{fig:VegEXTEND} presents 
the {\sc veg} of the inner loop, which sheds significantly more 
insights than the code would do.  %{\tt EXTEND\_do500},


\begin{figure}[t]
    \begin{tabular}{ll} \hspace{-5ex} % @{\hspace{0.15\textwidth}}
	\multirow{2}{*}[29ex]
	{   
   		\subfigure[$\mbox{~~~~~~~~~~~~~~~~}$]{
          	\label{fig:USR_ROio_EXTEND_do500} 
			\includegraphics[width=.44\textwidth]{\Figures/ROio_USR_EXTEND_do500}
	  	}
	} & {  \hspace{-31ex}
		\subfigure[$\mbox{~}$]{
          	\label{fig:VegEXTEND} 
			\includegraphics[width=.26\textwidth]{\Figures/VEG_EXTEND}
	  	}
	}
    \end{tabular} \vspace{15ex} 
\vspace{-2ex}
\hrule
\caption{(a) {\sc ro}$_i$ {\sc usr} \& (b) {\tt civ}'s {\sc veg} for {\tt EXTEND\_do500}.} 
% Iteration-level 
%\caption{ {\sc wf}, {\sc ro}, {rw} abstract-set summarization across consecutive regions and loops.}
\label{fig:Track} %
\end{figure}
  
\vspace{1ex}

{\bf 1. Foreign Variables Allowed in VEG.}
%
Sometimes two different variables are semantically the same {\sc civ},
for example in the sequence {\tt nxt=civ;$\ldots$ nxt=nxt+1;$\ldots$ civ=nxt},
shown in the {\sc veg} graph of Figure~\ref{fig:VegEXTEND}. 
It follows that we allow (foreign) variables such as {\tt nxt@3} to belong to
the {\sc veg} graph of {\tt civ} because (i) they semantically contribute to 
the flow of {\tt civ} values and (ii) the extended {\sc veg} still complies 
with the definition of a {\sc civ} variable (see Section~\ref{subsec:Background}), 
and because (ii) subscripts may contain both variables.  

%One can observe in Figure~\ref{fig:VegEXTEND} that variable {\tt nxt@3} semantically 
%contributes to the flow of {\tt civ} values, albeit it is a differently-named variable.
%It follows that we allow {\tt nxt@3} to belong to the {\sc veg} rooted in the $\mu$ 
%node {\tt civ@2}, especially since that {\sc veg} complies with the definition of a
%{\sc civ} variable, which was given at the end of Section~\ref{subsec:Background}.
%
The analysis treats {\tt nxt@3} as any other member of the {\tt civ} family of 
variables, except that summaries {\em cannot} be associated with foreign nodes,
because the latter do not necessarily encapsulate the correct control flow,
i.e., the $\gamma$ nodes of {\tt nxt@3} are not part of {\tt civ}'s {\sc veg} graph.
% $\gamma$ nodes of the {\sc civ} are not  It is incorrect to allow summaries 
%to be associated with foreign nodes such as {\tt nxt@3} because {\tt nxt@3}
%is not accounted in the $\gamma$ nodes of {\tt civ}s, and, as such, does not
%encapsulates the correct control flow.

\vspace{1ex}

{\bf 2. Filtering The Read Set}. The {\sc ro} ({\sc usr}) summary corresponding 
to iteration {\tt i} of loop {\tt EXTEND\_do500} is shown in 
Figure~\ref{fig:USR_ROio_EXTEND_do500}, where $\cup$, $-$ and $\#$ internal nodes 
correspond to set union, subtraction and gates, respectively.   
One can observe that {\sc ro}$_{i}$ is already nontrivial, and using it as
input to outer-level summarization would result in complex set expressions,
which would be ill-suited for computing independence, for example.
%
%contains both regular and {\sc civ}-based intervals (as leaves), and summarizing 
%the {\sc ro} set at loop level by the rules in Figure~\ref{fig:LoopAggreg} would 
%generate a very complex set expression, that would be unsuited to proving 
%independence and inferring accurate bounds for copy-in privatization.
%
We use instead a filtering technique based on the observation that
the contribution of {\sc ro}$_i$ to the loop-aggregated
{\sc ro} set cannot possibly belong to $\cup_{k=1}^{i-1}${\sc wf}$_k$.
The same property holds for the read-write set.

\begin{figure}[t]
\begin{scriptsize}
USR {\bf FREAD}($R$ : USR, $WF^p$ : LMAD) \vspace{1ex} \newline
$\mbox{ }\mbox{ }$// {\bf {\em Output:}} $R$ filtered-out of $WF^p$ terms. \vspace{1ex} \newline
$\mbox{ }\mbox{ }$Case $R$ of: \vspace{1ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ LMAD $L$: %\vspace{1ex} \newline
%$\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ 
{\bf IF} $\mathcal{F}(L - WF^p)$ {\bf THEN} $\emptyset$ {\bf ELSE} $L$ \vspace{1ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ $g\#A$: $g\#${\bf FREAD}($A$,$WF^p$)  \vspace{1ex} \newline   
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ $A \cup B$: {\bf FREAD}($A$,$WF^p$) $\cup$ {\bf FREAD}($B$,$WF^p$) \vspace{1ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ $A \cap B$: {\bf FREAD}($A$,$WF^p$) $\cap$ {\bf FREAD}($B$,$WF^p$) \vspace{1ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ $A - B$: {\bf FREAD}($A$,$\lfloor B \rfloor \cup WF^p$)   \vspace{1ex} \newline
$\mbox{ }\mbox{ }\mbox{ }\mbox{ }$ LOOP OR CALLSITE :   \vspace{1ex} \newline   
$\mbox{ }\mbox{ }\mbox{ }$ 
$\mbox{ }\mbox{ }\mbox{ }$ 
          {\bf IF}($\mathcal{F}$($\lceil R \rceil - WF^p$) THEN $\emptyset$ ELSE $R$
\end{scriptsize}
\hrule
\caption{ Read-Set Filtering Algorithm.}
\label{fig:ReadFilt} %
\end{figure}
 

Figure~\ref{fig:ReadFilt} shows the recursively-defined operator {\tt FREAD} that
filters out from the {\sc ro}$_i$ ({\sc rw}$_i$) set, the terms that are included 
into an interval underestimate of $\lfloor\cup_{k=1}^{i-1}${\sc wf}$_k\rfloor$, 
denoted $WF^p$.
%, from the {\sc ro}$_i$ and {\sc rw}$_i$ sets. 
The result summary replaces {\sc ro}$_i$ in Figure~\ref{fig:LoopAggreg}'s equations.

The algorithm pattern-matches the shape of the read-only summary:
If the current node is an interval $L$ then we extract a % of {\sc ro}$_i$ 
sufficient predicate for $L - WF^p = \emptyset$ by using the {\sc usr}-to-predicate
translation $\mathcal{F}$ mentioned in Section~\ref{subsec:Background}.   
If the predicate statically evaluates to {\tt true} then it is safe to 
filter $L$ out, otherwise $L$ (or $L-WF^p$) is kept.  

Similarly, if the current node, named $R$, is a loop or callsite node, 
then we check using $\mathcal{F}$ whether an interval overestimate of 
$R$ is included in $WF^p$; if so then $R$ is filtered out, otherwise 
it is kept.  
%
If $R$ is a gate or union or intersection node,
then each term is filtered and the results are composed back. 
%
If $R$ is a subtraction node $A-B$ then $A$ is filtered with the union 
of $WF^p$ and an interval underestimate of $B$. 

Filtering the read-only set depicted in Figure~\ref{fig:USR_ROio_EXTEND_do500}
with the computed $\lfloor \cup_{k=1}^{i-1}WF_k\rfloor={\tt [civ@1,civ@2-1]}$
results in the simple $RO'_{i} = {\tt\{i-1\}}$, where the problematic 
subtraction node, denoted by {\tt (**)} in Figure, has been simplified\footnote{
The algorithm has used the {\sc veg}-derived properties (i) ${\tt civ@1} < {\tt jts} \leq {\tt civ@2}$ 
and (ii) ${\tt M} \leq {\tt civ@1}$, where {\tt civ@1} is the {\sc civ} value 
just before entering the loop, and {\tt M} is the loop upper bound. Since 
{\tt civ}-values are monotonically increasing, it also follows: 
{\tt i $\leq$ M $\leq$ civ@1 $\leq$ civ@2 $<$ nxt@3.}
}
to $\emptyset$.%Similarly $RW'_{i} = \emptyset$.


\vspace{1ex}

{\bf 3. Output-Dependence Pattern.} 
%
{\tt EXTEND\_do400} exhibits per-iteration and partial-recurrence {\sc wf} 
sets of shape:\\
\noindent$\lceil WF_i\rceil = $ {\tt[civ@1,civ@8~~]},
$\lceil\cup_{k=1}^{i-1}WF_{k} \rceil = $ {\tt[civ,civ@1~~]},\\
\noindent$\lfloor WF_i\rfloor = $ {\tt[civ@1,civ@8-1]},
$\lfloor\cup_{k=1}^{i-1}WF_k \rfloor = $ {\tt[civ,civ@1-1]},\\
where {\tt civ@1} and {\tt civ@8} are the $\mu$ and back nodes of
the {\sc veg} associated to loop {\tt EXTEND\_do400} and variable {\tt civ}.
%
One can observe that output-dependencies may exist, since
the output-independence equation of Figure~\ref{fig:IndEq} is not satisfied:
$\lceil WF_{i}\rceil \cap \lceil \cup_{k=1}^{i-1}WF_{k} \rceil \equiv {\tt\{civ@1\}}\ne\emptyset$.
(This behavior is caused by a {\tt 0}-evolution path on which the array is updated,
and causes observable cross-iteration output dependencies.)

Still, dependencies exhibit a well-structured pattern that our 
implementation exploits. The key observations are: 
\begin{itemize}
    \item Indices belonging to $\lfloor{}WF_{j}\rfloor$ underestimate  
            do not result in cross dependencies:
        $\lfloor WF_{i}\rfloor\mbox{~}\cap\mbox{~}\cup_{k=1}^{i-1}\lfloor WF_{k}\rfloor~=~\emptyset$.

    \item All remaining indices, 
        $\lceil WF_{i}\rceil - \lfloor WF_{i}\rfloor = {\tt\{civ@8\}}$,
        are overwritten by the next {\sc civ}-increasing iteration:
        {\tt{}civ@8>civ@1} $\Rightarrow$
        {\tt\{civ@1\}} $\subseteq\lfloor WF_{i}\rfloor${\tt=[civ@1,civ@8-1]},
        %
        where {\tt\{civ@8\}} of the previous iteration was written as 
        {\tt\{civ@1\}} of the next {\sc civ}-increasing iteration.

    \item Last iteration increases the {\sc civ} value; this 
        holds for {\tt EXTEND\_do400} because {\sc civ} is 
        updated in an inner loop.
\end{itemize}

%verified at runtime via predicates.
If all properties hold, then the output-dependency pattern is resolved 
by privatizing the array (updates) and by copying out, at 
iteration's end, the indices belonging to the $\lfloor WF_i\rfloor$ 
{\em underestimate}, i.e., {\tt[civ@1,civ@8-1]}.
The last iteration copies in and out the overestimate (or updates directly),
and because it increases {\sc civ}, it is guaranteed to overwrite the 
uncommitted part of previous (non-increasing) iterations.
%
The last property requires runtime verification, during the {\sc civ}-value
(pre)computation, while the first two are derived statically for the current 
loop, but in general, they may also use runtime verification.

\vspace{1ex}
{\bf 4. Privatizable Stack.} Our analysis also extends (with
some modifications) to a stack-like access pattern in which
the {\sc civ} values are only piecewise monotonic.
This is discusses in Appendix~\ref{sec:Stack} in the
context of loop {\tt ACCEL\_do10} from {\tt tree} benchmark
that exhibits a privatizable stack. 


\subsection{CIV$_\mu$ Computation \& Implementation }
\label{subsect:CivImplem}

The independence results discussed in previous sections have referred 
so far to disambiguating array accesses that exhibit {\sc civ} 
subscripts, but not to the computation of the {\sc civ} values.
%
When the {\sc civ} variable is not privatisable in the context of the 
target loop, e.g., the loop in Figure~\ref{fig:codeActforCorrec}(b), 
the {\sc civ} is the source of cross-iteration flow dependencies: %in essence 
it is always updated in reduction-like statements, e.g., {\tt civ=civ+3*C(i)},
but it is read in arbitrary statements, e.g., in a subscript.


%While not always necessary~\cite{PaduaStackArr}, we  (pre)compute in %always 
While related approaches~\cite{PaduaDemDrInterproc,VEG} avoid this, we always pre-compute in
parallel, prior to loop execution, the {\sc civ}$_\mu$ values at the beginning 
of each (chunk of) iteration(s).
Reasons are threefold: {\em First}, in most cases %it is cleaner to do so, and 
the runtime overhead is small. {\em Second}, the {\sc civ} values may dictate
whether the loop is independent or not: For example,  the flow-independence
predicate of the loop in Figure~\ref{fig:codeActforCorrec}(b), 
${\tt N} \le {\tt Q}~\vee~{\tt civ@5} < {\tt M}$,
depends on {\tt civ@5}, while resolving statically the output 
dependencies of {\tt EXTEND\_do400}, depends, as
per Section~\ref{subsec:Track}, on establishing whether the last
iteration has increased the {\sc civ} value. 
%
{\em Third}, {\sc civ} values may flow in the computation
of data (rather than only subscripts), e.g., the recurrent
formula of the Sobol random number generator, used in
benchmark {\tt Pricing}  of Section~\ref{sec:EmpEval}.

{\sc civ}$_\mu$ values are (pre)computed by extracting the loop-slice
that contains the transitive closure of all statements 
that are necessary to compute the {\sc civ} variable
(we use the control-dependency graph). 
We privatize all arrays and scalars, including {\sc civ}, that are not 
read-only inside the slice, where each iteration copies-in the  indices 
of its $RO_i \cup RW_i$ set. Finally, an iteration-header statement is 
inserted to initialize {\sc civ} to the value just before the loop ({\tt Q}), 
and similarly, the end-of-iteration value minus {\tt Q} is saved into array 
{\tt CIVS}.  


\begin{figure}
\begin{colorcode}
//\mymath{slice for computing partial civ values}
DOALL i = M, N, 1      \$PRIVATIZED(civ,i)
  civ = Q
  IF ( C(i) .GT. 0 ) civ = civ + 3*C(i)
  CIVS(i-M+1) = civ - Q
ENDDOALL

//\mymath{SCAN(op {\tt{}+}, e, n, {\tt{}X}) \equiv} \{\mymath{e}, \mymath{e}+X(1),.., \mymath{e}+X(1)+..+X(n)\}
SCAN(op +, Q, N-M+1, CIVS)

//\mymath{civ values are plugged in the loop} 
DOALL i = M, N, 1      \$PRIVATIZED(civ,i)
  civ = CIVS(i-M+1)
  ... \mymath{rest of the loop code} 
ENDDOALL
\end{colorcode}
\vspace{-1ex}
\hrule
\vspace{-0.5ex}
\caption{ Parallel Computation of {\sc civ}$_\mu$ Values.}
\label{fig:CivSlice} %
\end{figure}

For example, Figure~\ref{fig:CivSlice} shows the {\sc civ} slice of the loop
in Figure~\ref{fig:codeActforCorrec}(a), where scalars {\tt i} and {\tt civ}
were privatized. The slice computes the per-iteration {\sc civ} increments 
and records them in {\tt CIVS}. Then {\tt SCAN}
computes in parallel the prefix sum of the {\tt CIVS} values, 
which are used each by one iteration of the parallelized loop.
%hence the {\sc civ}$_\mu$ values at the beginning of each iteration
%
Correctness requirements are {\em twofold}:  

{\em First}, all symbols that appear in the slice have been already proven 
to not introduce cross-iteration dependencies in the original loop; otherwise 
the computation of the slice might violate the semantics of the sequential execution. 

{\em Second}, {\sc civ} variables may appear in the slice only in reduction-like statements, 
such as {\tt civ=civ+1}.
If this holds than the final {\sc civ} value would correspond to an (additive) reduction, 
and hence, the intermediate {\sc civ}$_\mu$ values are safely computed via the parallel 
prefix sum of the values in {\tt CIVS}. 

If the latter does not hold, %than we are inside a vicious circle:
%the {\sc civ} computation depends on the {\sc civ} values 
%that we are about to compute. 
then we have a cycle between {\sc civ}-value computation and their use.   
We address such a case via a fixed-point implementation,
which (i) optimistically computes the {\sc civ} values as before, and then (ii) logically 
re-executes the slice on the computed-{\sc civ} values and checks (in parallel) 
whether the resulted value matches the input-value of the next iteration. 
%
If the check succeeds across all iterations, one can prove correctness by induction:
The first iteration is always correct. If the value at the end of the first iteration 
coincides with the input of the second, then the second iteration 
is correct, etc.%guaranteed to be correct, and so on.   

%We observe that the fix-pointed verification could be integrated in the 
%(parallel, optimistic) execution of the original loop: Rather than running the slice twice,
%we run the loop in a copy-in, copy-out policy, and if the verification fails then, 
%instead of copying-out the result arrays, the loop is re-executed sequentially.

Finally, {\sc civ}$_\mu$ input values are plugged in the 
parallel execution of the original loop: The arrays that were proven flow and output
independent are shared, i.e., not privatized.   For cases such as {\tt EXTEND\_do400},
that exhibit the special pattern of output dependencies, we adopt a strategy similar
to the one used for {\sc civ} computation: arrays such as {\tt X} are privatized,
their read-set {\sc ro}$_i$ $\cup$ {\sc rw}$_i$ is copied in, and the per-iteration 
{\sc wf} underestimate
is copied-out at the iteration's end.   Our technique is deeply integrated with the
underlying compiler framework: On the one hand, $\mathcal{F}$ uses the {\sc civ}-summarized
over and underestimates to derive sufficient-independence predicates.
%
On the other hand $\mathcal{F}$ is used in the summarization process, for example 
to simplify the {\sc ro}/{\sc rw} per-iteration sets, and thus to optimize the 
size of the data that needs to be copied in.  Similarly, Fourier-Motzkin-like
elimination reduces both loop indices and {\sc civ}s, i.e., when {\sc civ}'s 
bounds are derivable from its {\sc veg}. %in its corresponding {\sc veg}.



\section{Experimental Evaluation}
\label{sec:EmpEval}
    
\begin{table}[t] 
\centering
\scriptsize   
\begin{tabular}{|c|l|l|c|c|l|} \hline
\multicolumn{6}{|c|}{Properties of Benchmarks Exhibiting Important Loops That Use {\sc civ}s} \\ \hline
{\sc bench} & {\sc properties} & {\sc do loop}  & {\sc lsc}\%  & T$_{P/S}^L$(s) & {\sc type} \\ \hline
{\sc bdna}  &  T$_{P/S}$=.19/.65 s         & {\sc actfor\_500}  & 47.8 & .05/.31 & {\sc st-par}     \\ 
{\tt P=8}         &  {\sc sc}=87\%,{\sc ov}=0\%  & {\sc actfor\_240}  & 35.6 & .04/.23 & {\sc civ}$_{\tt{}AGG}$    \\ \hline 
             &  T$_{P/S}$=1.14/3.1 s         & {\sc gmttst\_120}  & 17.4 & .27/.54 & {\sc {\sc fi} {\sc o(1)}}   \\ 
{\sc nasa7}  &  {\sc sc}=98\%,{\sc ov}=0\%  & {\sc emit\_5}      & 13.6 & .09/.42 & {\sc civ}$_{\tt{}COMP}$   \\  % ,{\sc oi} {\sc o(n)}
{\tt P=8}        &                              &                      &                & & {\sc oi} {\sc o(n)}       \\
             &                              & {\sc btrtst\_120}  & 10.1 & .05/.31 & {\sc fi} {\sc o(1)}        \\ \hline
{\sc track}  &  T$_{P/S}$=6.6/16.8 s         & {\sc fptrak\_300}  & 52.8 & 3.6/8.9 & {\sc civ}$_{\tt{}COMP}$   \\ 
{\tt P=8}        &  {\sc sc}=97\%,{\sc ov}=45\%  & {\sc extend\_400}  & 43.9 & 2.3/7.4 & {\sc civ}$_{\tt{}COMP}$ \\ \hline 
{\sc tree}   &  T$_{P/S}$=12.8/59 s        & {\sc accel\_10}    & 91.2 & 7.6/54 & {\sc civ}$_{\tt{}AGG}$  \\            %58.6%53.5
{\tt P=8}        &  {\sc sc}=91\%,{\sc ov}=0\% &                    &      &          &                           \\ \hline 
{\sc price\_i} &  T$_{P/S}$=.29/2.0 s        & {\sc price\_i\_10} & 99   & .29/2.0  & {\sc civ}$_{\tt{}AGG}$ \\ 
{\tt P=8}          &  {\sc sc}=99\%,{\sc ov}=0\% &                    &      &          &                           \\ \hline 
{\sc price\_r} &  T$_{P/S}$=.17/.98 s          & {\sc price\_r\_10}& 99   & .17/.98  & {\sc civ}$_{\tt{}COMP}$ \\ 
{\tt P=8}          &  {\sc sc}=99\%,{\sc ov}=6.4\% &                  &      &          &                          \\ \hline 
\end{tabular}
\caption{ Characterization of Important CIV Loops.
%            Table's Layout Is Explained at the Start of Section~\ref{sec:EmpEval}. 
}
\label{tab:LoopBenchProps}
\end{table}


We evaluate our technique on a number of benchmarks containing loops that cover a significant
fraction of the total sequential runtime and that exhibit {\sc civ}-based accesses.
%
Table~\ref{tab:LoopBenchProps} characterizes several representative loops, named in 
the third column, and their corresponding benchmarks, named in the first column.
The reported parallel runtime were measured on {\tt P=8} cores.
%
%The last column shows the number of cores used for parallel execution. 

The second column shows 
(i)   the parallel and sequential runtime, T$_{P/S}$, measured in seconds,
(ii)  the percentage, {\sc sc}, of the sequential runtime that has been parallelized, and 
(iii) the overhead of the associated runtime tests, if any, which is represented as percentage of
        the total-parallel runtime. With our benchmarks, the only runtime test that introduces
        non-negligible overhead is the (pre)computation of {\sc civ} values, 
        denoted {\sc civ}$_{\tt{}COMP}$, and presented in Section~\ref{subsect:CivImplem}.   


%%% orig .37\textwidth  35ex
%\begin{figure*}[t]
%	\begin{tabular}{c c c} 
%\multicolumn{2}{}{} 
%	\multirow{2}{*}[37ex]
%	{      		%\hspace{-6ex}	
%               \includegraphics[width=.40\textwidth]{Figures/EmpRes/BenchParRes}\vspace{2ex}
%	} & { \hspace{-55ex}
%                    \includegraphics[width=.40\textwidth]{Figures/EmpRes/LoopParRes} \vspace{2ex}
%	} \\ 
%	\multirow{2}{*}[35ex]
%	{           
 %               \includegraphics[width=.40\textwidth]{Figures/EmpRes/LoopScalRes}
%	} & { \hspace{16ex}
%                \includegraphics[width=.40\textwidth]{Figures/EmpRes/TrackScal}
%	}
%\end{tabular} 
%\hrule
%\caption{ Benchmark and Loop-Level Normalized (Total) Parallel Runtime. Sequential Runtime is 1.}
%\label{fig:ParRuntime} %
%\end{figure*}


\begin{figure*}[t]
\begin{minipage}{1.2\columnwidth}
    \includegraphics[width=0.7\textwidth]{\Figures/EmpRes/BenchParRes}

      {\bf~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(a)} \vspace{2ex}

    \includegraphics[width=0.7\textwidth]{\Figures/EmpRes/LoopScalRes}
      
      {\bf~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(c)}\vspace{1ex}

\end{minipage}
\begin{minipage}{0.9\columnwidth}
    \includegraphics[width=0.9\textwidth]{\Figures/EmpRes/LoopParRes} 

      {\bf~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(b)}\vspace{2ex}

   \includegraphics[width=0.9\textwidth]{\Figures/EmpRes/TrackScal}
      
      {\bf~~~~~~~~~~~~~~~~~~~~~~~~~~~~~(d)}\vspace{1ex}

\end{minipage}
\hrule
\caption{ Benchmark and Loop-Level Normalized (Total) Parallel Runtime. Sequential Runtime is 1.}
\label{fig:ParRuntime} %
\end{figure*}


The third column shows the names of the most important loops,
the fourth shows their sequential coverage, {\sc lsc}, and the
fifth shows each loop's parallel and sequential runtime, T$_{P/S}^L$, in seconds.
%
We note that benchmarks {\tt bdna} and {\tt nasa7} have poor scalability, e.g., very small
performance gain from four to eight-core execution, because they exhibit
small (historical) datasets: low-granularity important loops that limits the 
profitability of parallelization.
%, which belong to the {\sc perfect-club} and {\sc spec2000} suites,
In addition, (i) loop {\tt GMTTST\_do120} 
is parallel but has loop-count three, and (ii) loop {\tt RESTAR\_do15},
which covers $9.3\%$ of the sequential runtime, uses {\sc io} operations,
and was run sequentially. 

The sixth column shows {\em how} the loop was classified parallel: {\sc fi} {\sc o(1)}
means that a predicate of runtime complexity {\sc o(1)} has validated loop's flow
independence at runtime. {\sc civ}$_{\tt{}AGG}$ indicates that {\sc civ}-based summarization 
was instrumental in proving independence statically, and {\sc civ}$_{\tt{}COMP}$ means that,
in addition to {\sc civ}$_{\tt{}AGG}$, the {\sc civ}$_\mu$ values were precomputed
at runtime, i.e., {\sc civ} was not privatizable.  {\sc st-par} indicates static
parallelization of a ``regular'' loop. 




\enlargethispage{\baselineskip}
\vspace{1ex}

{\bf Our Test Suite} consists of benchmarks {\tt bdna} and {\tt track} 
from {\sc perfect-club} suite, whose loops have been already discussed;
loop {\tt FPTRAK\_do300} from {\tt track} is similar to {\tt EXTEND\_do400},
%in Figure~\ref{fig:EXTEND}, 
and loop {\tt CORREC\_do401} is not shown because 
its sequential coverage is under $1\%$. 
%
Benchmark {\tt nasa7} is part of {\sc spec2000} suite, and its loop {\tt EMIT\_do5}
is similar to the one in Figure~\ref{fig:codeActforCorrec}(b).
%
Benchmark {\tt tree}~\cite{Treecode} is an implementation of the Barnes-Hut algorithm.
Its main loop, {\tt ACCEL\_do10}, exhibits the stack-access pattern of Figure~\ref{fig:Tree}(a),
and accounts for $91\%$ of the sequential runtime. The remaining $9\%$ corresponds to {\sc io}
operations and cannot be parallelized.
%
Finally,  {\tt price\_i/r} is a (simplified) kernel of a real-world application that
computes the price of a financial contract~\cite{LexiFiPricing}:   The difference
between the two versions is that (i) {\tt price\_i} uses an independent Sobol-random-number-generator
algorithm, i.e., computing the $i^{th}$ random number requires only the value of $i$,
and exhibits the {\sc civ} pattern of Figure~\ref{fig:codeActforCorrec}(a), 
while (ii) {\tt price\_r} uses a faster, recurrent formula that computes the 
$i^{th}$ random number based on the previous $(i-1)^{th}$ one. 
This means that in {\tt price\_r} the {\sc civ} is not used for indexing
but directly in the computation of Sobol numbers, i.e., a prallel prefix 
scan with exclusive-or operator {\tt xor}. 
Parallelization of {\tt price\_r} consists of (pre)computing
in parallel these {\sc civ}$_\mu$ values, in the manner of 
Section~\ref{subsect:CivImplem}.  
%
Because our implementation does not support {\tt xor},
we have manually turned the {\tt xor} into addition before 
compilation and then back to {\tt xor} after compilation. 

%\begin{figure}[t]
%	\begin{tabular}{l l l} 
%\multicolumn{2}{}{} 
%	\multirow{2}{*}[24ex]
%	{      		\hspace{-6ex}	
%                \includegraphics[width=.27\textwidth]{Figures/EmpRes/BenchParRes} \vspace{2ex}     
%	} & { \hspace{-39ex}
%                    \includegraphics[width=.27\textwidth]{Figures/EmpRes/LoopParRes} \vspace{2ex}
%	} \\ 
%	\multirow{2}{*}[24ex]
%	{           \hspace{-7ex} 
%                \includegraphics[width=.27\textwidth]{Figures/EmpRes/LoopScalRes}
%	} & {       \hspace{-5ex} 
%                \includegraphics[width=.25\textwidth]{Figures/EmpRes/TrackScalNew}
%	}
%\end{tabular}
%\caption{ Benchmark and Loop-Level Normalized Runtime and Scalability Results. }
%\label{fig:ParRuntime} %
%\end{figure}



%\vspace{1ex}


{\bf Experimental Methodology.} Our source-to-source compiler receives as
input a sequential {\tt Fortran77} program and automatically analysis loop-level 
parallelism and produces {\tt OpenMP} code.  The sequential and parallel 
code were compiled with {\tt gfortran}, option {\tt -O3}, and were run on 
a $16$-core {\tt AMD Opteron(TM) 6274} system with $128${\tt GB} memory. 
%Results were averaged for three independent runs.
%
%(-O4) and tested against IBM’s xlf compiler version
%13 on a 8 dual-core POWER 5+@1.9GHz, 32Gb-memory machine.

\enlargethispage{\baselineskip}

\vspace{1ex}

{\bf Results.} Figure~\ref{fig:ParRuntime}(a)~and~(b) show
the normalized-parallel runtime for each entire benchmark 
and {\sc civ} loop, respectively, where the {\sc civ}-computation
overhead is included in each bar.
%  where bars with names ending 
%in {\tt \_OV} show separately (only) the (non-negligible) runtime overhead of 
%(pre)computing the {\sc civ}s. 
%Note that the {\sc civ}-(pre)computation overhead is included
%for in the corresponding benchmark/loop bar.
%
Table~\ref{tab:LoopBenchProps} has shown that the {\sc civ}-computation slice 
(i) represents a small fraction $6\%$ of the execution of the 
{\tt pric\_r} benchmark,
%loop for {\tt PRICE\_R\_do10}, 
but (ii) it accounts for $45\%$ of the parallel execution time for {\tt track}.
The latter case is not surprising since the slice contains almost all statements 
of the original loop, i.e., {\tt EXTEND\_do400} and {\tt FPTRACK\_do300}, 
and both the loop and the slice are executed in parallel.    
Overall, for track we obtain a speedup of (only) $2.5$x on eight cores.


The less-than-optimal runtime for {\tt bdna}, {\tt nasa7}, and 
{\tt tree} was already explained, i.e., significant time spent in {\sc io},
small data sets. However, their loops show better results,
e.g., {\tt ACCEL\_do10} shows a  $7.1$x speedup on eight cores. % healthy

The loop-level scalability results of Figure~\ref{fig:ParRuntime}(c) 
hint that memory bandwidth is the main limitation. 
%For example, {\tt track}'s loops, overheads included, seem to scale well up to four cores,
%not so well on eight, and poor on sixteen. %(and its overhead has a similar behavior).
%The other-loops scalability is affected when passing from eight to sixteen
%cores, e.g., {\tt ACCEL\_do10} shows (only) $11$x speedup on sixteen cores.
%%, in comparison to $7.1$x on eight.

%(-O4) and tested against IBM’s xlf compiler version
%13 on a 8 dual-core POWER 5+@1.9GHz, 32Gb-memory machine.
To check this hypothesis we run {\sc track}, the most complex benchmark in our suite, 
on a bandwidth-friendlier system: an eight dual-core {\tt POWER 5+@1.9GHz} with $32$GB memory. 
The results, depicted in Figure~\ref{fig:ParRuntime}(d) show 
significantly-improved scalability up to sixteen cores for the 
total runtime. 
The {\sc civ}-computation overheads, also depicted in the chart, scales equally well.  
%and 
%the {\sc civ}-computation overhead of {\tt track}.
 
In summary, on eight processors we report an average benchmark-level
speedup of $4.33\times$, and an average {\sc civ}-loop speedup of $5.12\times$. 
The highest observed speedup, corresponding to {\tt price\_i}, is $12.5$x 
on sixteen cores.


\section{Related Work} 
\label{sec:RelWork}

Classical loop analysis examines each pair of read/write accesses, % dependence each
and models dependencies into linear systems of (in)equations that are solved
statically  via Gaussian-like elimination~\cite{BanerjeeIneqTest,FeautrierDataflow}. %Pugh92theomega
Such analysis  can drive powerful code transformations to optimize 
parallelism~\cite{PolyhedralOpt}, albeit in the narrow(er) domain 
in which subscripts, loop bounds, {\tt if} conditions are affine 
expressions of loop indices.

Several techniques have been proposed to handle irregular subscripts that have
the shape of closed-form formulas in the loop indices. For example, 
The Range Test~\cite{Blume94RangeTest} uses symbolic 
ranges to disambiguate a class of quadratic and  
exponential subscripts by exploiting the monotonicity of the read-write pair 
of subscripts.   A class of indirect-array subscripts is solved with 
an idiom-recognition method~\cite{PaduaDemDrInterproc}, where interprocedural 
analysis verifies the (assumed) monotonicity of the values stored in the 
index array.
%
Furthermore, Presburger arithmetic was extended to support
uninterpreted functions~\cite{Pugh98NonlinPresb}, and the irreducible-result
formula is executed at runtime to disambiguate a class of irregular
accesses. 

A different type of approach, which was found more effective in
solving larger loops, is to encode loop independence into one equation 
on abstract sets, for each array. The abstract set models the
memory references of the corresponding array and is computed 
via interprocedural summarization of 
accesses~\cite{SUIF,Moon99PredArrDataFlow,SummaryMonot,LMAD}.
While each of these methods covers some irregular accesses,
none of them handles {\sc civ}-subscripted loops,   
such as the ones analyzed in this paper.
%

A related  body of work presents (i) symbolic-algebra systems that, for example,  
compute upper and lower bounds of nonlinear expressions~\cite{Fahringer97EffSymb},
and (ii) techniques to characterizes scalar-value monotonicity within a 
loop~\cite{VEG,MonStmt}.
%, and (iii) generalized formulations of induction variables~\cite{Engelen04aunified}.

This solves only half of the problem, which   
corresponds to establishing the monotonic properties of {\sc civ}s.
This paper addresses the second challenge: 
how to build array summaries by using the {\sc civ}-related invariants,
where the immediate application is verifying loop independence. 
Related solutions analyze special cases of accesses:
%
For example, a pair of accesses of form  
\begin{small}{\tt\{X(CIV),X(CIV+d)\}}\end{small}, where {\tt d} is a constant,
can be disambiguated by reasoning in terms of the (range of the) cross-iteration 
evolution of the corresponding {\sc civ}~\cite{CohenBeyondMon}.
%
However, this does not address the case when a subscripts is affine
and the other uses {\sc civ}s, i.e., it might prove output, but not 
flow independence for the loop in Figure~\ref{fig:codeActforCorrec}(a), 
and it will not solve {\sc track}. 

Enhancing the idiom-recognition support~\cite{PaduaDemDrInterproc} %PaduaStackArr,
allows to relate better the {\sc civ} properties with the dependency test: 
For example, when the written {\sc civ} subscript matches the pattern 
of a consecutively-written single-index ({\sc cw-si}) array, it is often
possible to apply privatization, e.g., loop {\tt ACTFOR\_do240} in {\tt bdna}
and the stack access in Figure~\ref{fig:Tree}(a). %falls into a similar pattern.
%
However, loops such as the one in Figure~\ref{fig:codeActforCorrec}(a),
or {\sc track}, are unanalyzable because the subscripts are neither single 
indexed, nor consecutively written.  

In comparison, over/under-estimate summarization allows us,
intuitively, to reduce the problem to a known idiom, albeit
the code does not fall, strictly speaking, within that idiom.
Furthermore, other than {\sc veg}, our analysis does not rely
on pattern matching, but discovers non-trivial invariants that
enable the {\sc civ}-agnostic test either (i) to prove loop independence,
%e.g., Figure~\ref{fig:codeActforCorrec}(b), 
or (ii) to be tuned in a manner that resolves a specific pattern 
of dependencies, e.g., {\sc track}.

Finally, the value-evolution graph has been applied in the context of 
autoparallelization~\cite{VEG}.   The high-level difference is that there, 
subscripts are separated early into {\sc civ}-based and affine and they 
do not mix: they are summarized under different representations, 
and disambiguated via specialized dependency tests.
Analysis builds only accurate summaries, which may restrict the
effectiveness of dependency tests. 
%are identified through a {\sc cw-si}-like heuristic.
More specifically, there are not reported:
  (i)   symbolic, non-constant {\sc civ} evolutions,
        e.g., Fig~\ref{fig:codeActforCorrec}(b),
 (ii) the complex output-dependency pattern of {\sc track},
(iii) extraction of runtime predicates for {\sc civ}-dependence tests,
 (iv) prefix-sum precomputation of {\sc civ} values, in the
        cases when they are used as data, rather than for indexing,
  (v) privatization of stack-like accesses, 
 (vi) runtime and scalability results.

%Finally, four-core speedups for the {\sc civ} loops of {\tt bdna} and 
%{\tt track} benchmarks are reported in~\cite{CosPLDI}, but the technique 
%by which these {\sc civ} loops have been analyzed is not explained. 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section{Conclusions}
\label{sec:Concl}

This paper has presented an analysis that summarizes both 
affine and {\sc civ}-based subscripts under the same 
representation. 
%
The result summaries are {\sc civ} agnostic and can be used for 
various  purposes, e.g., dependence analysis or array {\sc ssa}. 
%
Our analysis seems less conservative than related approaches 
in that the algebra of under/over-estimates can exploit partial 
programming idioms, such as vectors in which elements are mostly 
pushed but some are overwritten. 

We have reported an automatic solution that is well integrated 
in the repertoire of a compiler that combines static and dynamic 
techniques to aggressively parallelize loops, and have 
demonstrated the viability of the approach via a systematic
evaluation of five real-world applications.

%\bibliographystyle{abbrvnat}
\bibliographystyle{abbrv}
%\softraggedright
\begin{small}
\bibliography{CIVpaper}
\end{small}



\end{document}


%%%%%%%%%%%%%%%%%%%%%%%%%
%%%% USELESS STUFF %%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%

